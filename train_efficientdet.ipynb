{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select from efficientnet backbone or resnet backbone\n",
    "backbone = \"efficientnet-b0\"\n",
    "scale = 1\n",
    "useBiFPN = True\n",
    "# scale==1: resolution 300\n",
    "# scale==2: resolution 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make data.Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlist:  16551\n",
      "vallist:  4952\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "# set your VOCdevkit path here.\n",
    "vocpath = \"../VOCdevkit/VOC2007\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "vocpath = \"../VOCdevkit/VOC2012\"\n",
    "train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath)\n",
    "\n",
    "train_img_list.extend(train_img_list2)\n",
    "train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "print(\"trainlist: \", len(train_img_list))\n",
    "print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 300*scale  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# Dataloaderに入れるデータセットファイル。\n",
    "# ゲットで叩くと画像とGTを前処理して出力してくれる。\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 300, 300])\n",
      "32\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 動作の確認\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
    "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define EfficientDet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.efficientdet import EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "layerc3: torch.Size([1, 40, 37, 37])\n",
      "layerc4: torch.Size([1, 80, 18, 18])\n",
      "layerc5: torch.Size([1, 320, 9, 9])\n",
      "layer size: torch.Size([1, 256, 37, 37])\n",
      "layer size: torch.Size([1, 256, 18, 18])\n",
      "layer size: torch.Size([1, 256, 9, 9])\n",
      "layer size: torch.Size([1, 256, 5, 5])\n",
      "layer size: torch.Size([1, 256, 3, 3])\n",
      "layer size: torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 8096, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "if scale==1:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [37, 18, 9, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "elif scale==2:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [75, 38, 19, 10, 5, 3],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264]*scale,  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315]*scale,  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "\n",
    "# test if net works\n",
    "net = EfficientDet(phase=\"train\", cfg=ssd_cfg, verbose=True, backbone=backbone, useBiFPN=useBiFPN)\n",
    "out = net(torch.rand([1,3,300,300]))\n",
    "print(out[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "net = EfficientDet(phase=\"train\", cfg=ssd_cfg, verbose=False, backbone=backbone, useBiFPN=useBiFPN)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientDet(\n",
      "  (layer0): Sequential(\n",
      "    (0): Conv2dStaticSamePadding(\n",
      "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "      (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "    )\n",
      "    (1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (conv6): Conv2d(320, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (toplayer): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (smooth1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (smooth2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (latlayer1): Conv2d(80, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (latlayer2): Conv2d(40, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device:\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('(train)')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('(val)')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iter {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/'+backbone+\"_\" + str(300*scale) + \"_\" + \n",
    "                       str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device: cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10 || Loss: 29.6469 || 10iter: 5.7856 sec.\n",
      "Iter 20 || Loss: 27.7266 || 10iter: 2.5724 sec.\n",
      "Iter 30 || Loss: 22.7666 || 10iter: 2.3900 sec.\n",
      "Iter 40 || Loss: 20.2654 || 10iter: 2.4015 sec.\n",
      "Iter 50 || Loss: 19.1678 || 10iter: 2.5208 sec.\n",
      "Iter 60 || Loss: 17.8210 || 10iter: 2.4121 sec.\n",
      "Iter 70 || Loss: 16.9245 || 10iter: 2.4418 sec.\n",
      "Iter 80 || Loss: 16.1897 || 10iter: 2.4140 sec.\n",
      "Iter 90 || Loss: 15.3084 || 10iter: 2.4101 sec.\n",
      "Iter 100 || Loss: 15.1703 || 10iter: 2.4014 sec.\n",
      "Iter 110 || Loss: 13.9548 || 10iter: 2.3977 sec.\n",
      "Iter 120 || Loss: 13.1043 || 10iter: 2.4734 sec.\n",
      "Iter 130 || Loss: 12.4970 || 10iter: 2.4313 sec.\n",
      "Iter 140 || Loss: 11.9180 || 10iter: 2.4973 sec.\n",
      "Iter 150 || Loss: 10.7936 || 10iter: 2.4723 sec.\n",
      "Iter 160 || Loss: 10.2956 || 10iter: 2.4339 sec.\n",
      "Iter 170 || Loss: 11.9987 || 10iter: 2.4240 sec.\n",
      "Iter 180 || Loss: 10.1595 || 10iter: 2.4878 sec.\n",
      "Iter 190 || Loss: 9.0999 || 10iter: 2.4149 sec.\n",
      "Iter 200 || Loss: 9.3735 || 10iter: 2.4926 sec.\n",
      "Iter 210 || Loss: 7.7816 || 10iter: 2.4450 sec.\n",
      "Iter 220 || Loss: 7.6061 || 10iter: 2.4439 sec.\n",
      "Iter 230 || Loss: 8.8519 || 10iter: 2.5068 sec.\n",
      "Iter 240 || Loss: 7.7002 || 10iter: 2.4359 sec.\n",
      "Iter 250 || Loss: 7.0196 || 10iter: 2.4498 sec.\n",
      "Iter 260 || Loss: 6.8062 || 10iter: 2.4601 sec.\n",
      "Iter 270 || Loss: 7.3335 || 10iter: 2.5477 sec.\n",
      "Iter 280 || Loss: 7.1677 || 10iter: 2.6119 sec.\n",
      "Iter 290 || Loss: 7.0741 || 10iter: 2.5362 sec.\n",
      "Iter 300 || Loss: 6.5247 || 10iter: 2.5334 sec.\n",
      "Iter 310 || Loss: 7.2043 || 10iter: 2.4222 sec.\n",
      "Iter 320 || Loss: 7.2233 || 10iter: 2.4321 sec.\n",
      "Iter 330 || Loss: 7.1088 || 10iter: 2.4494 sec.\n",
      "Iter 340 || Loss: 6.7640 || 10iter: 2.4340 sec.\n",
      "Iter 350 || Loss: 7.2225 || 10iter: 2.4194 sec.\n",
      "Iter 360 || Loss: 7.3903 || 10iter: 2.5503 sec.\n",
      "Iter 370 || Loss: 6.3046 || 10iter: 2.4288 sec.\n",
      "Iter 380 || Loss: 6.8197 || 10iter: 2.4315 sec.\n",
      "Iter 390 || Loss: 6.5198 || 10iter: 2.4326 sec.\n",
      "Iter 400 || Loss: 6.4928 || 10iter: 2.4053 sec.\n",
      "Iter 410 || Loss: 6.2727 || 10iter: 2.4916 sec.\n",
      "Iter 420 || Loss: 6.9002 || 10iter: 2.4694 sec.\n",
      "Iter 430 || Loss: 6.6029 || 10iter: 2.4848 sec.\n",
      "Iter 440 || Loss: 7.2432 || 10iter: 2.4644 sec.\n",
      "Iter 450 || Loss: 7.0458 || 10iter: 2.4253 sec.\n",
      "Iter 460 || Loss: 5.6699 || 10iter: 2.4601 sec.\n",
      "Iter 470 || Loss: 6.1264 || 10iter: 2.4367 sec.\n",
      "Iter 480 || Loss: 6.2665 || 10iter: 2.4465 sec.\n",
      "Iter 490 || Loss: 6.1826 || 10iter: 2.4232 sec.\n",
      "Iter 500 || Loss: 5.6577 || 10iter: 2.4671 sec.\n",
      "Iter 510 || Loss: 6.2202 || 10iter: 2.3719 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:5458.7002 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  131.5345 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 520 || Loss: 5.7972 || 10iter: 2.2280 sec.\n",
      "Iter 530 || Loss: 5.9533 || 10iter: 2.6162 sec.\n",
      "Iter 540 || Loss: 5.7764 || 10iter: 2.4010 sec.\n",
      "Iter 550 || Loss: 5.6730 || 10iter: 2.4691 sec.\n",
      "Iter 560 || Loss: 6.1579 || 10iter: 2.4377 sec.\n",
      "Iter 570 || Loss: 6.1104 || 10iter: 2.5114 sec.\n",
      "Iter 580 || Loss: 6.3605 || 10iter: 2.5322 sec.\n",
      "Iter 590 || Loss: 5.7485 || 10iter: 2.4497 sec.\n",
      "Iter 600 || Loss: 5.4334 || 10iter: 2.4007 sec.\n",
      "Iter 610 || Loss: 6.1932 || 10iter: 2.4010 sec.\n",
      "Iter 620 || Loss: 6.2750 || 10iter: 2.5245 sec.\n",
      "Iter 630 || Loss: 5.5166 || 10iter: 2.4488 sec.\n",
      "Iter 640 || Loss: 5.9516 || 10iter: 2.4754 sec.\n",
      "Iter 650 || Loss: 6.2545 || 10iter: 2.4722 sec.\n",
      "Iter 660 || Loss: 5.5212 || 10iter: 2.5185 sec.\n",
      "Iter 670 || Loss: 6.3648 || 10iter: 2.5049 sec.\n",
      "Iter 680 || Loss: 6.2307 || 10iter: 2.4118 sec.\n",
      "Iter 690 || Loss: 6.1977 || 10iter: 2.3956 sec.\n",
      "Iter 700 || Loss: 5.8070 || 10iter: 2.4754 sec.\n",
      "Iter 710 || Loss: 6.1094 || 10iter: 2.5198 sec.\n",
      "Iter 720 || Loss: 6.0915 || 10iter: 2.4162 sec.\n",
      "Iter 730 || Loss: 5.8599 || 10iter: 2.4363 sec.\n",
      "Iter 740 || Loss: 5.5504 || 10iter: 2.4337 sec.\n",
      "Iter 750 || Loss: 6.2124 || 10iter: 2.4346 sec.\n",
      "Iter 760 || Loss: 5.3055 || 10iter: 2.4143 sec.\n",
      "Iter 770 || Loss: 5.4552 || 10iter: 2.4392 sec.\n",
      "Iter 780 || Loss: 5.2544 || 10iter: 2.4271 sec.\n",
      "Iter 790 || Loss: 5.5677 || 10iter: 2.4949 sec.\n",
      "Iter 800 || Loss: 6.3410 || 10iter: 2.4134 sec.\n",
      "Iter 810 || Loss: 5.4767 || 10iter: 2.4217 sec.\n",
      "Iter 820 || Loss: 5.9707 || 10iter: 2.4438 sec.\n",
      "Iter 830 || Loss: 6.0001 || 10iter: 2.4367 sec.\n",
      "Iter 840 || Loss: 5.3524 || 10iter: 2.4121 sec.\n",
      "Iter 850 || Loss: 5.7823 || 10iter: 2.4232 sec.\n",
      "Iter 860 || Loss: 5.8247 || 10iter: 2.4940 sec.\n",
      "Iter 870 || Loss: 5.7410 || 10iter: 2.5010 sec.\n",
      "Iter 880 || Loss: 5.5319 || 10iter: 2.4671 sec.\n",
      "Iter 890 || Loss: 5.6665 || 10iter: 2.5809 sec.\n",
      "Iter 900 || Loss: 5.7166 || 10iter: 2.4492 sec.\n",
      "Iter 910 || Loss: 5.5872 || 10iter: 2.4274 sec.\n",
      "Iter 920 || Loss: 5.5161 || 10iter: 2.4598 sec.\n",
      "Iter 930 || Loss: 5.5896 || 10iter: 2.5045 sec.\n",
      "Iter 940 || Loss: 5.8203 || 10iter: 2.4302 sec.\n",
      "Iter 950 || Loss: 5.2177 || 10iter: 2.4344 sec.\n",
      "Iter 960 || Loss: 5.9388 || 10iter: 2.4054 sec.\n",
      "Iter 970 || Loss: 5.4173 || 10iter: 2.4483 sec.\n",
      "Iter 980 || Loss: 5.7206 || 10iter: 2.3948 sec.\n",
      "Iter 990 || Loss: 5.8024 || 10iter: 2.4011 sec.\n",
      "Iter 1000 || Loss: 5.8832 || 10iter: 2.4723 sec.\n",
      "Iter 1010 || Loss: 5.7052 || 10iter: 2.4527 sec.\n",
      "Iter 1020 || Loss: 5.3743 || 10iter: 2.5268 sec.\n",
      "Iter 1030 || Loss: 5.3763 || 10iter: 2.3385 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:2990.1862 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.7882 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 1040 || Loss: 5.4775 || 10iter: 2.9912 sec.\n",
      "Iter 1050 || Loss: 5.6048 || 10iter: 2.4648 sec.\n",
      "Iter 1060 || Loss: 5.4771 || 10iter: 2.4514 sec.\n",
      "Iter 1070 || Loss: 5.5806 || 10iter: 2.4716 sec.\n",
      "Iter 1080 || Loss: 5.4723 || 10iter: 2.4636 sec.\n",
      "Iter 1090 || Loss: 5.2116 || 10iter: 2.4322 sec.\n",
      "Iter 1100 || Loss: 5.4149 || 10iter: 2.5079 sec.\n",
      "Iter 1110 || Loss: 6.4393 || 10iter: 2.4902 sec.\n",
      "Iter 1120 || Loss: 5.0002 || 10iter: 2.3941 sec.\n",
      "Iter 1130 || Loss: 5.2053 || 10iter: 2.4851 sec.\n",
      "Iter 1140 || Loss: 5.0534 || 10iter: 2.4310 sec.\n",
      "Iter 1150 || Loss: 4.9530 || 10iter: 2.5165 sec.\n",
      "Iter 1160 || Loss: 5.3818 || 10iter: 2.4346 sec.\n",
      "Iter 1170 || Loss: 4.7616 || 10iter: 2.5481 sec.\n",
      "Iter 1180 || Loss: 5.2588 || 10iter: 2.4615 sec.\n",
      "Iter 1190 || Loss: 5.1422 || 10iter: 2.4058 sec.\n",
      "Iter 1200 || Loss: 5.1943 || 10iter: 2.4227 sec.\n",
      "Iter 1210 || Loss: 5.0407 || 10iter: 2.4333 sec.\n",
      "Iter 1220 || Loss: 5.5524 || 10iter: 2.4465 sec.\n",
      "Iter 1230 || Loss: 5.2062 || 10iter: 2.5187 sec.\n",
      "Iter 1240 || Loss: 5.2591 || 10iter: 2.4092 sec.\n",
      "Iter 1250 || Loss: 5.5180 || 10iter: 2.4149 sec.\n",
      "Iter 1260 || Loss: 5.2911 || 10iter: 2.4935 sec.\n",
      "Iter 1270 || Loss: 5.4496 || 10iter: 2.4093 sec.\n",
      "Iter 1280 || Loss: 5.4237 || 10iter: 2.4310 sec.\n",
      "Iter 1290 || Loss: 5.2557 || 10iter: 2.4401 sec.\n",
      "Iter 1300 || Loss: 5.2443 || 10iter: 2.5932 sec.\n",
      "Iter 1310 || Loss: 5.4499 || 10iter: 2.5007 sec.\n",
      "Iter 1320 || Loss: 5.1200 || 10iter: 2.4399 sec.\n",
      "Iter 1330 || Loss: 5.1044 || 10iter: 2.4171 sec.\n",
      "Iter 1340 || Loss: 4.9480 || 10iter: 2.4117 sec.\n",
      "Iter 1350 || Loss: 5.0296 || 10iter: 2.3975 sec.\n",
      "Iter 1360 || Loss: 4.9179 || 10iter: 2.4728 sec.\n",
      "Iter 1370 || Loss: 4.7952 || 10iter: 2.4129 sec.\n",
      "Iter 1380 || Loss: 5.3275 || 10iter: 2.5024 sec.\n",
      "Iter 1390 || Loss: 4.7692 || 10iter: 2.5061 sec.\n",
      "Iter 1400 || Loss: 5.8072 || 10iter: 2.4624 sec.\n",
      "Iter 1410 || Loss: 5.3269 || 10iter: 2.4396 sec.\n",
      "Iter 1420 || Loss: 5.3571 || 10iter: 2.4146 sec.\n",
      "Iter 1430 || Loss: 5.0237 || 10iter: 2.3972 sec.\n",
      "Iter 1440 || Loss: 4.9313 || 10iter: 2.4546 sec.\n",
      "Iter 1450 || Loss: 4.6501 || 10iter: 2.4814 sec.\n",
      "Iter 1460 || Loss: 5.5786 || 10iter: 2.5266 sec.\n",
      "Iter 1470 || Loss: 5.9119 || 10iter: 2.4819 sec.\n",
      "Iter 1480 || Loss: 5.2780 || 10iter: 2.3885 sec.\n",
      "Iter 1490 || Loss: 5.6730 || 10iter: 2.4846 sec.\n",
      "Iter 1500 || Loss: 4.8893 || 10iter: 2.4117 sec.\n",
      "Iter 1510 || Loss: 5.2232 || 10iter: 2.4343 sec.\n",
      "Iter 1520 || Loss: 5.3322 || 10iter: 2.5129 sec.\n",
      "Iter 1530 || Loss: 5.5193 || 10iter: 2.4596 sec.\n",
      "Iter 1540 || Loss: 4.9734 || 10iter: 2.4358 sec.\n",
      "Iter 1550 || Loss: 5.0995 || 10iter: 2.3295 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:2723.8853 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  129.0072 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 1560 || Loss: 5.0556 || 10iter: 3.1510 sec.\n",
      "Iter 1570 || Loss: 5.7468 || 10iter: 2.5689 sec.\n",
      "Iter 1580 || Loss: 5.1302 || 10iter: 2.4706 sec.\n",
      "Iter 1590 || Loss: 4.9565 || 10iter: 2.4414 sec.\n",
      "Iter 1600 || Loss: 5.3534 || 10iter: 2.5108 sec.\n",
      "Iter 1610 || Loss: 4.8265 || 10iter: 2.4903 sec.\n",
      "Iter 1620 || Loss: 5.2898 || 10iter: 2.4144 sec.\n",
      "Iter 1630 || Loss: 5.3032 || 10iter: 2.4462 sec.\n",
      "Iter 1640 || Loss: 4.9548 || 10iter: 2.4599 sec.\n",
      "Iter 1650 || Loss: 4.6178 || 10iter: 2.4220 sec.\n",
      "Iter 1660 || Loss: 4.6406 || 10iter: 2.4963 sec.\n",
      "Iter 1670 || Loss: 5.1028 || 10iter: 2.4278 sec.\n",
      "Iter 1680 || Loss: 4.8585 || 10iter: 2.4626 sec.\n",
      "Iter 1690 || Loss: 5.6053 || 10iter: 2.4297 sec.\n",
      "Iter 1700 || Loss: 5.1108 || 10iter: 2.4859 sec.\n",
      "Iter 1710 || Loss: 5.1819 || 10iter: 2.4520 sec.\n",
      "Iter 1720 || Loss: 4.8538 || 10iter: 2.4199 sec.\n",
      "Iter 1730 || Loss: 4.9126 || 10iter: 2.4372 sec.\n",
      "Iter 1740 || Loss: 4.7824 || 10iter: 2.4784 sec.\n",
      "Iter 1750 || Loss: 4.6280 || 10iter: 2.4144 sec.\n",
      "Iter 1760 || Loss: 5.4636 || 10iter: 2.3778 sec.\n",
      "Iter 1770 || Loss: 4.8501 || 10iter: 2.4601 sec.\n",
      "Iter 1780 || Loss: 4.9315 || 10iter: 2.4102 sec.\n",
      "Iter 1790 || Loss: 4.5920 || 10iter: 2.4767 sec.\n",
      "Iter 1800 || Loss: 4.8333 || 10iter: 2.4231 sec.\n",
      "Iter 1810 || Loss: 4.6291 || 10iter: 2.4013 sec.\n",
      "Iter 1820 || Loss: 4.7047 || 10iter: 2.4322 sec.\n",
      "Iter 1830 || Loss: 4.8621 || 10iter: 2.4397 sec.\n",
      "Iter 1840 || Loss: 4.8540 || 10iter: 2.4331 sec.\n",
      "Iter 1850 || Loss: 5.3080 || 10iter: 2.4153 sec.\n",
      "Iter 1860 || Loss: 5.0576 || 10iter: 2.4566 sec.\n",
      "Iter 1870 || Loss: 5.0551 || 10iter: 2.4582 sec.\n",
      "Iter 1880 || Loss: 4.4743 || 10iter: 2.4320 sec.\n",
      "Iter 1890 || Loss: 4.8096 || 10iter: 2.4283 sec.\n",
      "Iter 1900 || Loss: 5.3165 || 10iter: 2.4639 sec.\n",
      "Iter 1910 || Loss: 4.6494 || 10iter: 2.4960 sec.\n",
      "Iter 1920 || Loss: 4.7682 || 10iter: 2.4501 sec.\n",
      "Iter 1930 || Loss: 4.5964 || 10iter: 2.3941 sec.\n",
      "Iter 1940 || Loss: 5.0455 || 10iter: 2.5028 sec.\n",
      "Iter 1950 || Loss: 4.9369 || 10iter: 2.4086 sec.\n",
      "Iter 1960 || Loss: 5.1085 || 10iter: 2.3914 sec.\n",
      "Iter 1970 || Loss: 4.6974 || 10iter: 2.4636 sec.\n",
      "Iter 1980 || Loss: 4.5850 || 10iter: 2.4269 sec.\n",
      "Iter 1990 || Loss: 5.2431 || 10iter: 2.4579 sec.\n",
      "Iter 2000 || Loss: 5.1474 || 10iter: 2.4931 sec.\n",
      "Iter 2010 || Loss: 5.5807 || 10iter: 2.4322 sec.\n",
      "Iter 2020 || Loss: 4.7478 || 10iter: 2.4556 sec.\n",
      "Iter 2030 || Loss: 4.6088 || 10iter: 2.5160 sec.\n",
      "Iter 2040 || Loss: 5.1160 || 10iter: 2.4243 sec.\n",
      "Iter 2050 || Loss: 4.3516 || 10iter: 2.4452 sec.\n",
      "Iter 2060 || Loss: 4.7651 || 10iter: 2.3851 sec.\n",
      "Iter 2070 || Loss: 4.4969 || 10iter: 2.3299 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:2566.8389 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.3019 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 2080 || Loss: 4.6960 || 10iter: 3.7446 sec.\n",
      "Iter 2090 || Loss: 5.0204 || 10iter: 2.4385 sec.\n",
      "Iter 2100 || Loss: 4.4877 || 10iter: 2.4202 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2110 || Loss: 4.5929 || 10iter: 2.4400 sec.\n",
      "Iter 2120 || Loss: 4.5427 || 10iter: 2.4375 sec.\n",
      "Iter 2130 || Loss: 4.3950 || 10iter: 2.4435 sec.\n",
      "Iter 2140 || Loss: 4.6723 || 10iter: 2.4848 sec.\n",
      "Iter 2150 || Loss: 4.8161 || 10iter: 2.5091 sec.\n",
      "Iter 2160 || Loss: 4.5712 || 10iter: 2.4049 sec.\n",
      "Iter 2170 || Loss: 5.4162 || 10iter: 2.4287 sec.\n",
      "Iter 2180 || Loss: 4.9540 || 10iter: 2.4788 sec.\n",
      "Iter 2190 || Loss: 4.6401 || 10iter: 2.4451 sec.\n",
      "Iter 2200 || Loss: 5.4198 || 10iter: 2.4532 sec.\n",
      "Iter 2210 || Loss: 4.6213 || 10iter: 2.5303 sec.\n",
      "Iter 2220 || Loss: 4.5708 || 10iter: 2.4666 sec.\n",
      "Iter 2230 || Loss: 4.9106 || 10iter: 2.4277 sec.\n",
      "Iter 2240 || Loss: 4.5620 || 10iter: 2.4446 sec.\n",
      "Iter 2250 || Loss: 4.4419 || 10iter: 2.4708 sec.\n",
      "Iter 2260 || Loss: 4.9993 || 10iter: 2.4447 sec.\n",
      "Iter 2270 || Loss: 4.4786 || 10iter: 2.4677 sec.\n",
      "Iter 2280 || Loss: 4.8669 || 10iter: 2.4061 sec.\n",
      "Iter 2290 || Loss: 4.7898 || 10iter: 2.4336 sec.\n",
      "Iter 2300 || Loss: 4.1915 || 10iter: 2.4760 sec.\n",
      "Iter 2310 || Loss: 4.5893 || 10iter: 2.4217 sec.\n",
      "Iter 2320 || Loss: 4.7227 || 10iter: 2.4973 sec.\n",
      "Iter 2330 || Loss: 4.8436 || 10iter: 2.4804 sec.\n",
      "Iter 2340 || Loss: 4.6976 || 10iter: 2.4128 sec.\n",
      "Iter 2350 || Loss: 5.0815 || 10iter: 2.4926 sec.\n",
      "Iter 2360 || Loss: 4.6129 || 10iter: 2.4707 sec.\n",
      "Iter 2370 || Loss: 4.9123 || 10iter: 2.4811 sec.\n",
      "Iter 2380 || Loss: 4.6938 || 10iter: 2.4332 sec.\n",
      "Iter 2390 || Loss: 5.3737 || 10iter: 2.4128 sec.\n",
      "Iter 2400 || Loss: 4.9801 || 10iter: 2.4349 sec.\n",
      "Iter 2410 || Loss: 4.4913 || 10iter: 2.3887 sec.\n",
      "Iter 2420 || Loss: 4.3450 || 10iter: 2.4734 sec.\n",
      "Iter 2430 || Loss: 4.5429 || 10iter: 2.4129 sec.\n",
      "Iter 2440 || Loss: 4.8376 || 10iter: 2.4748 sec.\n",
      "Iter 2450 || Loss: 4.6067 || 10iter: 2.4106 sec.\n",
      "Iter 2460 || Loss: 4.4034 || 10iter: 2.5190 sec.\n",
      "Iter 2470 || Loss: 5.1497 || 10iter: 2.4562 sec.\n",
      "Iter 2480 || Loss: 4.9348 || 10iter: 2.4650 sec.\n",
      "Iter 2490 || Loss: 4.9056 || 10iter: 2.4968 sec.\n",
      "Iter 2500 || Loss: 4.0166 || 10iter: 2.4270 sec.\n",
      "Iter 2510 || Loss: 4.8590 || 10iter: 2.4144 sec.\n",
      "Iter 2520 || Loss: 4.3401 || 10iter: 2.4990 sec.\n",
      "Iter 2530 || Loss: 4.7567 || 10iter: 2.4585 sec.\n",
      "Iter 2540 || Loss: 4.2414 || 10iter: 2.4258 sec.\n",
      "Iter 2550 || Loss: 4.5664 || 10iter: 2.4673 sec.\n",
      "Iter 2560 || Loss: 5.6762 || 10iter: 2.4107 sec.\n",
      "Iter 2570 || Loss: 5.0784 || 10iter: 2.4633 sec.\n",
      "Iter 2580 || Loss: 5.2221 || 10iter: 2.3985 sec.\n",
      "Iter 2590 || Loss: 5.0835 || 10iter: 2.1950 sec.\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:2460.7719 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.5825 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 2600 || Loss: 5.0531 || 10iter: 4.2734 sec.\n",
      "Iter 2610 || Loss: 5.2104 || 10iter: 2.3899 sec.\n",
      "Iter 2620 || Loss: 4.6602 || 10iter: 2.4563 sec.\n",
      "Iter 2630 || Loss: 4.5525 || 10iter: 2.4176 sec.\n",
      "Iter 2640 || Loss: 4.4008 || 10iter: 2.4217 sec.\n",
      "Iter 2650 || Loss: 4.4647 || 10iter: 2.4918 sec.\n",
      "Iter 2660 || Loss: 4.2198 || 10iter: 2.4458 sec.\n",
      "Iter 2670 || Loss: 5.0353 || 10iter: 2.4675 sec.\n",
      "Iter 2680 || Loss: 4.7423 || 10iter: 2.5149 sec.\n",
      "Iter 2690 || Loss: 4.5951 || 10iter: 2.4415 sec.\n",
      "Iter 2700 || Loss: 4.7000 || 10iter: 2.4184 sec.\n",
      "Iter 2710 || Loss: 5.2687 || 10iter: 2.4053 sec.\n",
      "Iter 2720 || Loss: 4.4832 || 10iter: 2.4307 sec.\n",
      "Iter 2730 || Loss: 4.3184 || 10iter: 2.4938 sec.\n",
      "Iter 2740 || Loss: 4.6129 || 10iter: 2.4960 sec.\n",
      "Iter 2750 || Loss: 4.5941 || 10iter: 2.5559 sec.\n",
      "Iter 2760 || Loss: 4.7321 || 10iter: 2.4163 sec.\n",
      "Iter 2770 || Loss: 4.2945 || 10iter: 2.5142 sec.\n",
      "Iter 2780 || Loss: 4.8749 || 10iter: 2.5343 sec.\n",
      "Iter 2790 || Loss: 4.5400 || 10iter: 2.5481 sec.\n",
      "Iter 2800 || Loss: 4.6525 || 10iter: 2.4074 sec.\n",
      "Iter 2810 || Loss: 4.8769 || 10iter: 2.3975 sec.\n",
      "Iter 2820 || Loss: 4.7883 || 10iter: 2.4385 sec.\n",
      "Iter 2830 || Loss: 4.7778 || 10iter: 2.4501 sec.\n",
      "Iter 2840 || Loss: 4.5178 || 10iter: 2.4049 sec.\n",
      "Iter 2850 || Loss: 4.3105 || 10iter: 2.4102 sec.\n",
      "Iter 2860 || Loss: 4.0050 || 10iter: 2.4864 sec.\n",
      "Iter 2870 || Loss: 4.2049 || 10iter: 2.4511 sec.\n",
      "Iter 2880 || Loss: 4.3816 || 10iter: 2.4399 sec.\n",
      "Iter 2890 || Loss: 4.4216 || 10iter: 2.4222 sec.\n",
      "Iter 2900 || Loss: 4.1202 || 10iter: 2.4304 sec.\n",
      "Iter 2910 || Loss: 4.3712 || 10iter: 2.4807 sec.\n",
      "Iter 2920 || Loss: 4.2748 || 10iter: 2.4669 sec.\n",
      "Iter 2930 || Loss: 4.5606 || 10iter: 2.4491 sec.\n",
      "Iter 2940 || Loss: 4.3585 || 10iter: 2.4598 sec.\n",
      "Iter 2950 || Loss: 4.3002 || 10iter: 2.4613 sec.\n",
      "Iter 2960 || Loss: 4.8435 || 10iter: 2.4246 sec.\n",
      "Iter 2970 || Loss: 4.7941 || 10iter: 2.4051 sec.\n",
      "Iter 2980 || Loss: 4.8041 || 10iter: 2.4436 sec.\n",
      "Iter 2990 || Loss: 4.4449 || 10iter: 2.3868 sec.\n",
      "Iter 3000 || Loss: 4.4342 || 10iter: 2.4178 sec.\n",
      "Iter 3010 || Loss: 4.1928 || 10iter: 2.4019 sec.\n",
      "Iter 3020 || Loss: 4.6546 || 10iter: 2.4171 sec.\n",
      "Iter 3030 || Loss: 4.3630 || 10iter: 2.4616 sec.\n",
      "Iter 3040 || Loss: 4.3482 || 10iter: 2.4392 sec.\n",
      "Iter 3050 || Loss: 4.4900 || 10iter: 2.4684 sec.\n",
      "Iter 3060 || Loss: 4.5755 || 10iter: 2.5089 sec.\n",
      "Iter 3070 || Loss: 4.3187 || 10iter: 2.5212 sec.\n",
      "Iter 3080 || Loss: 4.8399 || 10iter: 2.4564 sec.\n",
      "Iter 3090 || Loss: 4.4556 || 10iter: 2.4337 sec.\n",
      "Iter 3100 || Loss: 4.9130 || 10iter: 2.3356 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:2389.8328 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.5212 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 3110 || Loss: 4.4894 || 10iter: 2.0453 sec.\n",
      "Iter 3120 || Loss: 4.7804 || 10iter: 2.6465 sec.\n",
      "Iter 3130 || Loss: 4.4445 || 10iter: 2.4372 sec.\n",
      "Iter 3140 || Loss: 5.0256 || 10iter: 2.4345 sec.\n",
      "Iter 3150 || Loss: 4.6696 || 10iter: 2.4046 sec.\n",
      "Iter 3160 || Loss: 4.3680 || 10iter: 2.4860 sec.\n",
      "Iter 3170 || Loss: 4.4619 || 10iter: 2.5559 sec.\n",
      "Iter 3180 || Loss: 4.9651 || 10iter: 2.4156 sec.\n",
      "Iter 3190 || Loss: 4.7693 || 10iter: 2.4253 sec.\n",
      "Iter 3200 || Loss: 4.1175 || 10iter: 2.4429 sec.\n",
      "Iter 3210 || Loss: 4.1820 || 10iter: 2.4929 sec.\n",
      "Iter 3220 || Loss: 4.6455 || 10iter: 2.4019 sec.\n",
      "Iter 3230 || Loss: 4.4722 || 10iter: 2.3945 sec.\n",
      "Iter 3240 || Loss: 5.0548 || 10iter: 2.4620 sec.\n",
      "Iter 3250 || Loss: 4.4922 || 10iter: 2.4580 sec.\n",
      "Iter 3260 || Loss: 4.5237 || 10iter: 2.4352 sec.\n",
      "Iter 3270 || Loss: 4.3181 || 10iter: 2.5147 sec.\n",
      "Iter 3280 || Loss: 4.8958 || 10iter: 2.4102 sec.\n",
      "Iter 3290 || Loss: 4.5976 || 10iter: 2.3954 sec.\n",
      "Iter 3300 || Loss: 3.9564 || 10iter: 2.4777 sec.\n",
      "Iter 3310 || Loss: 4.4829 || 10iter: 2.4040 sec.\n",
      "Iter 3320 || Loss: 5.1180 || 10iter: 2.4238 sec.\n",
      "Iter 3330 || Loss: 4.3436 || 10iter: 2.3934 sec.\n",
      "Iter 3340 || Loss: 4.3116 || 10iter: 2.4138 sec.\n",
      "Iter 3350 || Loss: 4.1616 || 10iter: 2.4399 sec.\n",
      "Iter 3360 || Loss: 4.2961 || 10iter: 2.4861 sec.\n",
      "Iter 3370 || Loss: 4.7069 || 10iter: 2.4216 sec.\n",
      "Iter 3380 || Loss: 4.5988 || 10iter: 2.5412 sec.\n",
      "Iter 3390 || Loss: 4.1208 || 10iter: 2.4912 sec.\n",
      "Iter 3400 || Loss: 4.7683 || 10iter: 2.5729 sec.\n",
      "Iter 3410 || Loss: 4.4075 || 10iter: 2.4056 sec.\n",
      "Iter 3420 || Loss: 4.1827 || 10iter: 2.4387 sec.\n",
      "Iter 3430 || Loss: 4.3532 || 10iter: 2.5124 sec.\n",
      "Iter 3440 || Loss: 4.3999 || 10iter: 2.4896 sec.\n",
      "Iter 3450 || Loss: 3.8324 || 10iter: 2.4499 sec.\n",
      "Iter 3460 || Loss: 4.3101 || 10iter: 2.4548 sec.\n",
      "Iter 3470 || Loss: 4.4688 || 10iter: 2.5184 sec.\n",
      "Iter 3480 || Loss: 4.4567 || 10iter: 2.5074 sec.\n",
      "Iter 3490 || Loss: 4.2571 || 10iter: 2.4434 sec.\n",
      "Iter 3500 || Loss: 4.6788 || 10iter: 2.4773 sec.\n",
      "Iter 3510 || Loss: 4.9786 || 10iter: 2.4556 sec.\n",
      "Iter 3520 || Loss: 4.3531 || 10iter: 2.4426 sec.\n",
      "Iter 3530 || Loss: 4.4676 || 10iter: 2.4806 sec.\n",
      "Iter 3540 || Loss: 4.4868 || 10iter: 2.4691 sec.\n",
      "Iter 3550 || Loss: 4.6447 || 10iter: 2.4076 sec.\n",
      "Iter 3560 || Loss: 3.9142 || 10iter: 2.4362 sec.\n",
      "Iter 3570 || Loss: 4.4961 || 10iter: 2.4210 sec.\n",
      "Iter 3580 || Loss: 4.4914 || 10iter: 2.4345 sec.\n",
      "Iter 3590 || Loss: 4.3602 || 10iter: 2.4171 sec.\n",
      "Iter 3600 || Loss: 4.5980 || 10iter: 2.4199 sec.\n",
      "Iter 3610 || Loss: 3.9687 || 10iter: 2.4774 sec.\n",
      "Iter 3620 || Loss: 5.2675 || 10iter: 2.3297 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:2318.8568 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.5569 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 3630 || Loss: 4.6208 || 10iter: 2.8310 sec.\n",
      "Iter 3640 || Loss: 4.4557 || 10iter: 2.4760 sec.\n",
      "Iter 3650 || Loss: 4.3404 || 10iter: 2.4599 sec.\n",
      "Iter 3660 || Loss: 4.4802 || 10iter: 2.4478 sec.\n",
      "Iter 3670 || Loss: 4.6127 || 10iter: 2.4542 sec.\n",
      "Iter 3680 || Loss: 4.1256 || 10iter: 2.4606 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3690 || Loss: 3.8857 || 10iter: 2.4995 sec.\n",
      "Iter 3700 || Loss: 4.5967 || 10iter: 2.4365 sec.\n",
      "Iter 3710 || Loss: 4.3485 || 10iter: 2.3928 sec.\n",
      "Iter 3720 || Loss: 3.4526 || 10iter: 2.4347 sec.\n",
      "Iter 3730 || Loss: 4.5667 || 10iter: 2.4212 sec.\n",
      "Iter 3740 || Loss: 4.9599 || 10iter: 2.4614 sec.\n",
      "Iter 3750 || Loss: 4.0177 || 10iter: 2.4907 sec.\n",
      "Iter 3760 || Loss: 4.4663 || 10iter: 2.4096 sec.\n",
      "Iter 3770 || Loss: 4.4304 || 10iter: 2.4432 sec.\n",
      "Iter 3780 || Loss: 4.1937 || 10iter: 2.4066 sec.\n",
      "Iter 3790 || Loss: 4.2452 || 10iter: 2.4045 sec.\n",
      "Iter 3800 || Loss: 4.7228 || 10iter: 2.4305 sec.\n",
      "Iter 3810 || Loss: 4.3935 || 10iter: 2.4974 sec.\n",
      "Iter 3820 || Loss: 4.1687 || 10iter: 2.4436 sec.\n",
      "Iter 3830 || Loss: 4.1716 || 10iter: 2.4119 sec.\n",
      "Iter 3840 || Loss: 4.7033 || 10iter: 2.4222 sec.\n",
      "Iter 3850 || Loss: 4.6345 || 10iter: 2.4044 sec.\n",
      "Iter 3860 || Loss: 4.1382 || 10iter: 2.3899 sec.\n",
      "Iter 3870 || Loss: 4.1831 || 10iter: 2.4243 sec.\n",
      "Iter 3880 || Loss: 4.4006 || 10iter: 2.5000 sec.\n",
      "Iter 3890 || Loss: 4.3995 || 10iter: 2.4798 sec.\n",
      "Iter 3900 || Loss: 4.5067 || 10iter: 2.4747 sec.\n",
      "Iter 3910 || Loss: 4.3668 || 10iter: 2.4403 sec.\n",
      "Iter 3920 || Loss: 4.2369 || 10iter: 2.4474 sec.\n",
      "Iter 3930 || Loss: 4.9579 || 10iter: 2.4126 sec.\n",
      "Iter 3940 || Loss: 5.4346 || 10iter: 2.4539 sec.\n",
      "Iter 3950 || Loss: 3.9698 || 10iter: 2.5002 sec.\n",
      "Iter 3960 || Loss: 4.3590 || 10iter: 2.4555 sec.\n",
      "Iter 3970 || Loss: 4.6320 || 10iter: 2.4494 sec.\n",
      "Iter 3980 || Loss: 4.5555 || 10iter: 2.5084 sec.\n",
      "Iter 3990 || Loss: 3.9448 || 10iter: 2.3976 sec.\n",
      "Iter 4000 || Loss: 4.0447 || 10iter: 2.4253 sec.\n",
      "Iter 4010 || Loss: 4.4067 || 10iter: 2.4766 sec.\n",
      "Iter 4020 || Loss: 4.2138 || 10iter: 2.4482 sec.\n",
      "Iter 4030 || Loss: 4.7749 || 10iter: 2.4049 sec.\n",
      "Iter 4040 || Loss: 4.6472 || 10iter: 2.4374 sec.\n",
      "Iter 4050 || Loss: 5.1710 || 10iter: 2.4538 sec.\n",
      "Iter 4060 || Loss: 4.5196 || 10iter: 2.4490 sec.\n",
      "Iter 4070 || Loss: 4.6689 || 10iter: 2.4070 sec.\n",
      "Iter 4080 || Loss: 4.1257 || 10iter: 2.3986 sec.\n",
      "Iter 4090 || Loss: 4.5088 || 10iter: 2.4024 sec.\n",
      "Iter 4100 || Loss: 4.0606 || 10iter: 2.4055 sec.\n",
      "Iter 4110 || Loss: 4.1754 || 10iter: 2.4273 sec.\n",
      "Iter 4120 || Loss: 4.5263 || 10iter: 2.5247 sec.\n",
      "Iter 4130 || Loss: 4.1963 || 10iter: 2.4461 sec.\n",
      "Iter 4140 || Loss: 4.0420 || 10iter: 2.3527 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:2290.5895 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  128.2237 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 4150 || Loss: 3.7080 || 10iter: 3.3110 sec.\n",
      "Iter 4160 || Loss: 4.4931 || 10iter: 2.4444 sec.\n",
      "Iter 4170 || Loss: 4.3009 || 10iter: 2.4726 sec.\n",
      "Iter 4180 || Loss: 4.5071 || 10iter: 2.5128 sec.\n",
      "Iter 4190 || Loss: 4.2587 || 10iter: 2.4379 sec.\n",
      "Iter 4200 || Loss: 3.9407 || 10iter: 2.5416 sec.\n",
      "Iter 4210 || Loss: 4.2151 || 10iter: 2.4433 sec.\n",
      "Iter 4220 || Loss: 4.0614 || 10iter: 2.4507 sec.\n",
      "Iter 4230 || Loss: 4.5148 || 10iter: 2.4169 sec.\n",
      "Iter 4240 || Loss: 4.6926 || 10iter: 2.4301 sec.\n",
      "Iter 4250 || Loss: 4.7518 || 10iter: 2.4733 sec.\n",
      "Iter 4260 || Loss: 4.7823 || 10iter: 2.4694 sec.\n",
      "Iter 4270 || Loss: 4.2450 || 10iter: 2.4621 sec.\n",
      "Iter 4280 || Loss: 4.1196 || 10iter: 2.4899 sec.\n",
      "Iter 4290 || Loss: 4.6790 || 10iter: 2.5165 sec.\n",
      "Iter 4300 || Loss: 4.5018 || 10iter: 2.4518 sec.\n",
      "Iter 4310 || Loss: 4.3221 || 10iter: 2.4686 sec.\n",
      "Iter 4320 || Loss: 4.4913 || 10iter: 2.4883 sec.\n",
      "Iter 4330 || Loss: 4.7442 || 10iter: 2.4663 sec.\n",
      "Iter 4340 || Loss: 4.7917 || 10iter: 2.4600 sec.\n",
      "Iter 4350 || Loss: 4.0211 || 10iter: 2.3999 sec.\n",
      "Iter 4360 || Loss: 4.5502 || 10iter: 2.4442 sec.\n",
      "Iter 4370 || Loss: 4.1983 || 10iter: 2.4770 sec.\n",
      "Iter 4380 || Loss: 4.5269 || 10iter: 2.4317 sec.\n",
      "Iter 4390 || Loss: 4.0058 || 10iter: 2.4989 sec.\n",
      "Iter 4400 || Loss: 4.0087 || 10iter: 2.4814 sec.\n",
      "Iter 4410 || Loss: 4.4326 || 10iter: 2.4883 sec.\n",
      "Iter 4420 || Loss: 3.9614 || 10iter: 2.4912 sec.\n",
      "Iter 4430 || Loss: 4.3028 || 10iter: 2.4204 sec.\n",
      "Iter 4440 || Loss: 4.2075 || 10iter: 2.5408 sec.\n",
      "Iter 4450 || Loss: 4.2469 || 10iter: 2.4571 sec.\n",
      "Iter 4460 || Loss: 4.0268 || 10iter: 2.5215 sec.\n",
      "Iter 4470 || Loss: 5.0967 || 10iter: 2.4660 sec.\n",
      "Iter 4480 || Loss: 4.3266 || 10iter: 2.4212 sec.\n",
      "Iter 4490 || Loss: 4.2138 || 10iter: 2.4767 sec.\n",
      "Iter 4500 || Loss: 4.7208 || 10iter: 2.4545 sec.\n",
      "Iter 4510 || Loss: 4.2881 || 10iter: 2.4969 sec.\n",
      "Iter 4520 || Loss: 4.0897 || 10iter: 2.4377 sec.\n",
      "Iter 4530 || Loss: 3.8563 || 10iter: 2.4309 sec.\n",
      "Iter 4540 || Loss: 3.9984 || 10iter: 2.4546 sec.\n",
      "Iter 4550 || Loss: 4.1587 || 10iter: 2.3920 sec.\n",
      "Iter 4560 || Loss: 4.3645 || 10iter: 2.4233 sec.\n",
      "Iter 4570 || Loss: 4.1189 || 10iter: 2.4161 sec.\n",
      "Iter 4580 || Loss: 4.1734 || 10iter: 2.4913 sec.\n",
      "Iter 4590 || Loss: 4.1174 || 10iter: 2.3992 sec.\n",
      "Iter 4600 || Loss: 4.4548 || 10iter: 2.4665 sec.\n",
      "Iter 4610 || Loss: 4.6319 || 10iter: 2.4287 sec.\n",
      "Iter 4620 || Loss: 3.8555 || 10iter: 2.4624 sec.\n",
      "Iter 4630 || Loss: 3.7792 || 10iter: 2.4191 sec.\n",
      "Iter 4640 || Loss: 4.7210 || 10iter: 2.5352 sec.\n",
      "Iter 4650 || Loss: 4.1422 || 10iter: 2.4399 sec.\n",
      "Iter 4660 || Loss: 4.1517 || 10iter: 2.3528 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:2253.7469 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  129.1439 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 4670 || Loss: 4.0557 || 10iter: 3.9502 sec.\n",
      "Iter 4680 || Loss: 4.6237 || 10iter: 2.5373 sec.\n",
      "Iter 4690 || Loss: 4.5821 || 10iter: 2.5352 sec.\n",
      "Iter 4700 || Loss: 4.3425 || 10iter: 2.4963 sec.\n",
      "Iter 4710 || Loss: 4.0680 || 10iter: 2.4572 sec.\n",
      "Iter 4720 || Loss: 3.8167 || 10iter: 2.4288 sec.\n",
      "Iter 4730 || Loss: 3.9678 || 10iter: 2.4161 sec.\n",
      "Iter 4740 || Loss: 4.5428 || 10iter: 2.4197 sec.\n",
      "Iter 4750 || Loss: 4.1550 || 10iter: 2.4743 sec.\n",
      "Iter 4760 || Loss: 4.3897 || 10iter: 2.4152 sec.\n",
      "Iter 4770 || Loss: 4.1180 || 10iter: 2.4135 sec.\n",
      "Iter 4780 || Loss: 4.5080 || 10iter: 2.3926 sec.\n",
      "Iter 4790 || Loss: 4.2758 || 10iter: 2.4156 sec.\n",
      "Iter 4800 || Loss: 4.4100 || 10iter: 2.4519 sec.\n",
      "Iter 4810 || Loss: 4.6823 || 10iter: 2.4345 sec.\n",
      "Iter 4820 || Loss: 4.1222 || 10iter: 2.4333 sec.\n",
      "Iter 4830 || Loss: 4.5908 || 10iter: 2.4831 sec.\n",
      "Iter 4840 || Loss: 4.1927 || 10iter: 2.4544 sec.\n",
      "Iter 4850 || Loss: 4.2704 || 10iter: 2.4784 sec.\n",
      "Iter 4860 || Loss: 4.4417 || 10iter: 2.4758 sec.\n",
      "Iter 4870 || Loss: 3.9775 || 10iter: 2.4160 sec.\n",
      "Iter 4880 || Loss: 4.0929 || 10iter: 2.4080 sec.\n",
      "Iter 4890 || Loss: 3.8073 || 10iter: 2.4092 sec.\n",
      "Iter 4900 || Loss: 3.9192 || 10iter: 2.4017 sec.\n",
      "Iter 4910 || Loss: 4.9156 || 10iter: 2.4614 sec.\n",
      "Iter 4920 || Loss: 4.2548 || 10iter: 2.4755 sec.\n",
      "Iter 4930 || Loss: 4.4301 || 10iter: 2.5024 sec.\n",
      "Iter 4940 || Loss: 4.0896 || 10iter: 2.4730 sec.\n",
      "Iter 4950 || Loss: 4.2122 || 10iter: 2.4621 sec.\n",
      "Iter 4960 || Loss: 4.1123 || 10iter: 2.4747 sec.\n",
      "Iter 4970 || Loss: 4.2243 || 10iter: 2.4215 sec.\n",
      "Iter 4980 || Loss: 3.7330 || 10iter: 2.5023 sec.\n",
      "Iter 4990 || Loss: 4.4686 || 10iter: 2.4252 sec.\n",
      "Iter 5000 || Loss: 3.9752 || 10iter: 2.4453 sec.\n",
      "Iter 5010 || Loss: 4.3267 || 10iter: 2.4697 sec.\n",
      "Iter 5020 || Loss: 4.5149 || 10iter: 2.4752 sec.\n",
      "Iter 5030 || Loss: 4.0839 || 10iter: 2.4451 sec.\n",
      "Iter 5040 || Loss: 4.3278 || 10iter: 2.4672 sec.\n",
      "Iter 5050 || Loss: 4.5067 || 10iter: 2.4088 sec.\n",
      "Iter 5060 || Loss: 4.3202 || 10iter: 2.4131 sec.\n",
      "Iter 5070 || Loss: 4.3788 || 10iter: 2.4484 sec.\n",
      "Iter 5080 || Loss: 4.8443 || 10iter: 2.4394 sec.\n",
      "Iter 5090 || Loss: 4.3150 || 10iter: 2.5170 sec.\n",
      "Iter 5100 || Loss: 4.3052 || 10iter: 2.4155 sec.\n",
      "Iter 5110 || Loss: 4.3947 || 10iter: 2.4376 sec.\n",
      "Iter 5120 || Loss: 4.5800 || 10iter: 2.4370 sec.\n",
      "Iter 5130 || Loss: 3.9164 || 10iter: 2.4780 sec.\n",
      "Iter 5140 || Loss: 3.7312 || 10iter: 2.4553 sec.\n",
      "Iter 5150 || Loss: 4.0514 || 10iter: 2.4388 sec.\n",
      "Iter 5160 || Loss: 5.0737 || 10iter: 2.4318 sec.\n",
      "Iter 5170 || Loss: 4.5227 || 10iter: 2.4079 sec.\n",
      "Iter 5180 || Loss: 5.9016 || 10iter: 2.1922 sec.\n",
      "-------------\n",
      "(val)\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:2213.4306 ||Epoch_VAL_Loss:626.6353\n",
      "timer:  148.9707 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 5190 || Loss: 4.3657 || 10iter: 4.5675 sec.\n",
      "Iter 5200 || Loss: 4.0339 || 10iter: 2.4196 sec.\n",
      "Iter 5210 || Loss: 4.8253 || 10iter: 2.5185 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
